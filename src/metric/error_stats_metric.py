#!/usr/bin/env python3
"""
Error Statistics Analysis Script

This script performs comprehensive error pattern analysis on de novo sequencing results
by comparing predicted sequences with ground truth. It requires robustness metric files
and database search results as input to generate detailed error statistics.

Pipeline Position:
    1. denovo_process.py → Generate summary CSV files
    2. robustness_metric.py → Generate robustness metrics
    3. [THIS SCRIPT] Error analysis using robustness + DB search files → Error statistics

Dependencies:
    - Robustness metric CSV files from robustness_metric.py
    - summary_merged.csv file from denovo_process.py (contains all tool predictions + ground truth)
    - The summary_merged.csv serves as the unified summary input file

Error Types Analyzed:
    - Substitution errors (amino acid replacements)
    - Insertion errors (extra amino acids)
    - Deletion errors (missing amino acids) 
    - Permutation errors (sequence rearrangements)
    - Mass shift errors

Outputs:
    - Detailed error pattern statistics per tool
    - Error distribution plots and visualizations
    - JSON and CSV reports with error breakdowns

Usage:
    python error_stats_metric.py --robustness_file /path/to/robustness.csv --summary_file /path/to/summary_merged.csv --output_dir /path/to/error_analysis
    
Note: The --summary_file should point to the summary_merged.csv file generated by denovo_process.py
"""

import os
import argparse
import csv
import numpy as np
import pandas as pd
import yaml
import json
import logging
from typing import Dict, Optional, Any, List
import matplotlib
from sklearn import metrics
from collections import Counter


class ErrorStatsConfigLoader:
    """Configuration loader for Error Stats Metric parameters."""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize configuration loader.
        
        Args:
            config_path: Path to configuration file. If None, uses default path.
        """
        if config_path is None:
            # Default config path is in the src/config directory
            script_dir = os.path.dirname(os.path.abspath(__file__))
            # Go up one level from metric to src, then into config
            src_dir = os.path.dirname(script_dir)
            config_path = os.path.join(src_dir, 'config', 'error_stats_metric.yaml')
        
        self.config_path = config_path
        self.config = self._load_config()
        
        # Get project root directory for resolving relative paths
        self.project_root = self._get_project_root()
    
    def _load_config(self) -> Dict[str, Any]:
        """Load configuration from YAML file."""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"Loaded configuration from {self.config_path}")
            return config
        except FileNotFoundError:
            print(f"Warning: Configuration file {self.config_path} not found. Using fallback defaults.")
            return self._get_fallback_config()
        except yaml.YAMLError as e:
            print(f"Error parsing YAML config: {e}. Using fallback defaults.")
            return self._get_fallback_config()
    
    def _get_project_root(self) -> str:
        """Get project root directory."""
        # Start from script directory and go up to find project root
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # Go up two levels: metric -> src -> project_root
        project_root = os.path.dirname(os.path.dirname(script_dir))
        return project_root
    
    def _resolve_path(self, relative_path: str) -> str:
        """Resolve relative path to absolute path based on project root."""
        if os.path.isabs(relative_path):
            return relative_path
        return os.path.join(self.project_root, relative_path)

    def _get_fallback_config(self) -> Dict[str, Any]:
        """Get fallback configuration if config file is not available."""
        return {
            'tool_names': [
                'CasanovoV1', 'CasanovoV2', 'CasanovoV3', 'DeepNovo', 
                'InstaNovo', 'InstaNovoPlus', 'PepNet', 'pi-HelixNovo'
            ],
            'analysis_parameters': {
                'show_progress': True,
                'save_plots': True,
                'plot_format': 'pdf'
            },
            'default_paths': {
                'input_base': 'analysis',
                'output_base': 'analysis_results'
            }
        }
    
    @property
    def tool_names(self) -> List[str]:
        """Get supported tool names."""
        return self.config.get('tool_names', [])
    
    @property
    def analysis_parameters(self) -> Dict[str, Any]:
        """Get analysis parameters."""
        return self.config.get('analysis_parameters', {})
    
    @property
    def default_paths(self) -> Dict[str, str]:
        """Get default file paths (resolved to absolute paths)."""
        paths = self.config.get('default_paths', {})
        resolved_paths = {}
        for key, path in paths.items():
            resolved_paths[key] = self._resolve_path(path)
        return resolved_paths
    
    @property
    def input_files(self) -> Dict[str, str]:
        """Get input file patterns."""
        return self.config.get('input_files', {})
    
    @property
    def output_files(self) -> Dict[str, str]:
        """Get output file patterns."""
        return self.config.get('output_files', {})
    
    @property
    def plot_config(self) -> Dict[str, Any]:
        """Get plot configuration."""
        return self.config.get('plot_config', {})
    
    @property
    def error_types(self) -> List[str]:
        """Get error types to analyze."""
        return self.config.get('error_types', [])
    
    @property
    def statistics_config(self) -> Dict[str, Any]:
        """Get statistics configuration."""
        return self.config.get('statistics_config', {})
    
    @property
    def logging_config(self) -> Dict[str, Any]:
        """Get logging configuration."""
        return self.config.get('logging_config', {})


# Global configuration loader
_error_stats_config_loader = None

def get_error_stats_config_loader(config_path: Optional[str] = None) -> ErrorStatsConfigLoader:
    """Get global configuration loader instance."""
    global _error_stats_config_loader
    if _error_stats_config_loader is None or config_path is not None:
        _error_stats_config_loader = ErrorStatsConfigLoader(config_path)
    return _error_stats_config_loader


class ErrorStatsAnalyzer:
    """Error statistics analyzer for de novo sequencing results."""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize the analyzer.
        
        Args:
            config_path: Path to configuration file
        """
        # Load configuration
        self.config = get_error_stats_config_loader(config_path)
        
        # Set up logging
        self._setup_logging()
        
        # Set up matplotlib
        self._setup_matplotlib()
        
        # Set parameters from config
        analysis_params = self.config.analysis_parameters
        self.show_progress = analysis_params.get('show_progress', True)
        self.save_plots = analysis_params.get('save_plots', True)
        self.plot_format = analysis_params.get('plot_format', 'pdf')
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        log_config = self.config.logging_config
        
        log_level = getattr(logging, log_config.get('level', 'INFO').upper())
        log_format = log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        logging.basicConfig(level=log_level, format=log_format)
        
        # Suppress matplotlib logging
        logging.getLogger('matplotlib').setLevel(logging.ERROR)
        
        self.logger = logging.getLogger(__name__)
    
    def _setup_matplotlib(self) -> None:
        """Setup matplotlib configuration."""
        plot_config = self.config.plot_config
        backend = plot_config.get('backend', 'pdf')
        
        matplotlib.use(backend)
        
        # Import pyplot after setting backend
        global plt
        from matplotlib import pyplot as plt
        
        # Set default parameters
        plt.rcParams['figure.figsize'] = plot_config.get('figure_size', [12, 8])
        plt.rcParams['figure.dpi'] = plot_config.get('dpi', 300)
        plt.rcParams['font.size'] = plot_config.get('font_size', 12)
    
    def are_permutation(self, str1: str, str2: str) -> bool:
        """
        Check if two strings are permutations of each other.
        
        Args:
            str1: First string
            str2: Second string
            
        Returns:
            True if strings are permutations (excluding exact matches)
        """
        n1 = len(str1)
        n2 = len(str2)
        
        if str1 == str2:
            return False
        if n1 != n2:
            return False
        
        # Sort and compare
        a = sorted(str1)
        b = sorted(str2)
        
        return a == b
    
    def calculate_sequence_errors(self, predicted: str, true: str) -> Dict[str, int]:
        """
        Calculate different types of sequence errors.
        
        Args:
            predicted: Predicted sequence
            true: True/reference sequence
            
        Returns:
            Dictionary with error counts by type
        """
        errors = {
            'substitution': 0,
            'insertion': 0,
            'deletion': 0,
            'permutation': 0,
            'exact_match': 0
        }
        
        if predicted == true:
            errors['exact_match'] = 1
            return errors
        
        if self.are_permutation(predicted, true):
            errors['permutation'] = 1
            return errors
        
        # Use Levenshtein distance for other error types
        try:
            from fast_diff_match_patch import diff
            diffs = diff(true, predicted)
            
            for op, text in diffs:
                if op == 1:  # insertion
                    errors['insertion'] += len(text)
                elif op == -1:  # deletion
                    errors['deletion'] += len(text)
                else:  # equal or substitution
                    # Count character-level substitutions
                    min_len = min(len(predicted), len(true))
                    for i in range(min_len):
                        if i < len(predicted) and i < len(true) and predicted[i] != true[i]:
                            errors['substitution'] += 1
        except ImportError:
            # Fallback to simple character comparison
            self.logger.warning("fast_diff_match_patch not available, using simple comparison")
            min_len = min(len(predicted), len(true))
            max_len = max(len(predicted), len(true))
            
            for i in range(min_len):
                if predicted[i] != true[i]:
                    errors['substitution'] += 1
            
            if len(predicted) > len(true):
                errors['insertion'] += len(predicted) - len(true)
            elif len(true) > len(predicted):
                errors['deletion'] += len(true) - len(predicted)
        
        return errors
    
    def error_stats(self, summary_df: pd.DataFrame, output_dir: str) -> Dict[str, Any]:
        """
        Calculate comprehensive error statistics.
        
        Args:
            summary_df: DataFrame with prediction and truth columns
            output_dir: Output directory for results
            
        Returns:
            Dictionary with error statistics
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # Initialize results
        results = {
            'tool_stats': {},
            'overall_stats': {},
            'error_breakdown': {}
        }
        
        tools = self.config.tool_names
        
        for tool in tools:
            if tool not in summary_df.columns:
                self.logger.warning(f"Tool {tool} not found in data")
                continue
            
            tool_results = {
                'total_predictions': 0,
                'error_counts': Counter(),
                'accuracy_metrics': {}
            }
            
            # Filter valid predictions for this tool
            tool_data = summary_df[summary_df[tool].notna()].copy()
            
            if tool_data.empty:
                self.logger.warning(f"No valid data for tool {tool}")
                continue
            
            tool_results['total_predictions'] = len(tool_data)
            
            # Calculate errors for each prediction
            for idx, row in tool_data.iterrows():
                predicted = str(row[tool]).strip()
                true_seq = str(row.get('truth', row.get('sequence', ''))).strip()
                
                if not predicted or not true_seq:
                    continue
                
                errors = self.calculate_sequence_errors(predicted, true_seq)
                
                for error_type, count in errors.items():
                    tool_results['error_counts'][error_type] += count
            
            # Calculate accuracy metrics
            exact_matches = tool_results['error_counts']['exact_match']
            total = tool_results['total_predictions']
            
            if total > 0:
                tool_results['accuracy_metrics'] = {
                    'exact_match_accuracy': exact_matches / total,
                    'error_rate': (total - exact_matches) / total,
                    'permutation_rate': tool_results['error_counts']['permutation'] / total,
                    'substitution_rate': tool_results['error_counts']['substitution'] / total,
                    'insertion_rate': tool_results['error_counts']['insertion'] / total,
                    'deletion_rate': tool_results['error_counts']['deletion'] / total
                }
            
            results['tool_stats'][tool] = tool_results
            
            self.logger.info(f"Processed {total} predictions for {tool}")
        
        # Calculate overall statistics
        self._calculate_overall_stats(results)
        
        # Save results
        self._save_results(results, output_dir)
        
        # Generate plots if enabled
        if self.save_plots:
            self._generate_plots(results, output_dir)
        
        return results
    
    def _calculate_overall_stats(self, results: Dict[str, Any]) -> None:
        """Calculate overall statistics across all tools."""
        overall = {
            'total_tools': len(results['tool_stats']),
            'avg_accuracy': 0.0,
            'best_tool': '',
            'worst_tool': '',
            'error_distribution': Counter()
        }
        
        if not results['tool_stats']:
            results['overall_stats'] = overall
            return
        
        accuracies = []
        best_accuracy = 0.0
        worst_accuracy = 1.0
        
        for tool, stats in results['tool_stats'].items():
            accuracy = stats['accuracy_metrics'].get('exact_match_accuracy', 0.0)
            accuracies.append(accuracy)
            
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                overall['best_tool'] = tool
            
            if accuracy < worst_accuracy:
                worst_accuracy = accuracy
                overall['worst_tool'] = tool
            
            # Aggregate error counts
            for error_type, count in stats['error_counts'].items():
                overall['error_distribution'][error_type] += count
        
        overall['avg_accuracy'] = np.mean(accuracies) if accuracies else 0.0
        results['overall_stats'] = overall
    
    def _save_results(self, results: Dict[str, Any], output_dir: str) -> None:
        """Save results to files."""
        output_files = self.config.output_files
        prefix = output_files.get('error_stats_prefix', 'error_stats')
        
        # Save detailed results as JSON
        json_file = os.path.join(output_dir, f"{prefix}_detailed.json")
        with open(json_file, 'w') as f:
            # Convert Counter objects to dict for JSON serialization
            json_results = self._prepare_for_json(results)
            json.dump(json_results, f, indent=2)
        
        # Save summary as CSV
        csv_file = os.path.join(output_dir, f"{prefix}_summary.csv")
        summary_data = []
        
        for tool, stats in results['tool_stats'].items():
            row = {'Tool': tool}
            row.update(stats['accuracy_metrics'])
            row['Total_Predictions'] = stats['total_predictions']
            summary_data.append(row)
        
        if summary_data:
            pd.DataFrame(summary_data).to_csv(csv_file, index=False)
        
        self.logger.info(f"Results saved to {output_dir}")
    
    def _prepare_for_json(self, obj: Any) -> Any:
        """Prepare object for JSON serialization."""
        if isinstance(obj, dict):
            return {key: self._prepare_for_json(value) for key, value in obj.items()}
        elif isinstance(obj, Counter):
            return dict(obj)
        elif isinstance(obj, (list, tuple)):
            return [self._prepare_for_json(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        else:
            return obj
    
    def _generate_plots(self, results: Dict[str, Any], output_dir: str) -> None:
        """Generate visualization plots."""
        try:
            plot_prefix = self.config.output_files.get('plot_prefix', 'error_analysis')
            
            # Plot 1: Accuracy comparison
            self._plot_accuracy_comparison(results, output_dir, plot_prefix)
            
            # Plot 2: Error type distribution
            self._plot_error_distribution(results, output_dir, plot_prefix)
            
            self.logger.info(f"Plots saved to {output_dir}")
            
        except Exception as e:
            self.logger.error(f"Error generating plots: {e}")
    
    def _plot_accuracy_comparison(self, results: Dict[str, Any], output_dir: str, prefix: str) -> None:
        """Plot accuracy comparison across tools."""
        tools = []
        accuracies = []
        
        for tool, stats in results['tool_stats'].items():
            tools.append(tool)
            accuracies.append(stats['accuracy_metrics'].get('exact_match_accuracy', 0.0))
        
        if not tools:
            return
        
        plt.figure(figsize=self.config.plot_config.get('figure_size', [12, 8]))
        bars = plt.bar(tools, accuracies)
        plt.title('Exact Match Accuracy by Tool')
        plt.xlabel('Tool')
        plt.ylabel('Accuracy')
        plt.xticks(rotation=45)
        plt.ylim(0, 1)
        
        # Add value labels on bars
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"{prefix}_accuracy.{self.plot_format}"))
        plt.close()
    
    def _plot_error_distribution(self, results: Dict[str, Any], output_dir: str, prefix: str) -> None:
        """Plot error type distribution."""
        error_dist = results['overall_stats'].get('error_distribution', {})
        
        if not error_dist:
            return
        
        error_types = list(error_dist.keys())
        counts = list(error_dist.values())
        
        plt.figure(figsize=self.config.plot_config.get('figure_size', [12, 8]))
        plt.pie(counts, labels=error_types, autopct='%1.1f%%')
        plt.title('Error Type Distribution')
        plt.axis('equal')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"{prefix}_error_distribution.{self.plot_format}"))
        plt.close()
    
    def analyze_files(self, robustness_file: str, summary_file: str, output_dir: str) -> Dict[str, Any]:
        """
        Analyze error statistics from input files.
        
        Args:
            robustness_file: Path to robustness metric CSV file
            summary_file: Path to summary merged CSV file
            output_dir: Output directory for results
            
        Returns:
            Error analysis results
        """
        try:
            # Load data files
            self.logger.info(f"Loading robustness data from {robustness_file}")
            df1 = pd.read_csv(robustness_file)
            
            self.logger.info(f"Loading summary data from {summary_file}")
            df2 = pd.read_csv(summary_file)
            
            # Combine dataframes (implement specific merge logic based on data structure)
            # This is a placeholder - adjust based on actual data structure
            if 'sequence' in df1.columns and 'sequence' in df2.columns:
                combined_df = pd.merge(df1, df2, on='sequence', how='outer')
            else:
                # Simple concatenation if no common key
                combined_df = pd.concat([df1, df2], axis=1)
            
            self.logger.info(f"Combined dataset has {len(combined_df)} rows")
            
            # Run error analysis
            results = self.error_stats(combined_df, output_dir)
            
            return results
            
        except Exception as e:
            self.logger.error(f"Error analyzing files: {e}")
            raise


def main():
    """Main function to run Error Stats analysis."""
    parser = argparse.ArgumentParser(description='Error Stats Metric Script')
    
    # Load config first to get default values
    config = get_error_stats_config_loader()
    default_paths = config.default_paths
    input_files = config.input_files
    
    parser.add_argument('--robustness_file', type=str,
                       default=os.path.join(default_paths.get('input_base', 'analysis'), 
                                          input_files.get('robustness_metric', 'robustness_metric.csv')),
                       help='Path to robustness metric CSV file')
    parser.add_argument('--summary_file', type=str,
                       default=os.path.join(default_paths.get('input_base', 'analysis'),
                                          input_files.get('summary_merged', 'summary_merged.csv')),
                       help='Path to summary merged CSV file')
    parser.add_argument('--output_dir', type=str,
                       default=default_paths.get('output_base', 'analysis_results'),
                       help='Output directory for analysis results')
    parser.add_argument('--config_path', type=str,
                       help='Path to configuration YAML file')
    parser.add_argument('--plot_format', type=str,
                       default=config.analysis_parameters.get('plot_format', 'pdf'),
                       choices=['pdf', 'png', 'svg'],
                       help='Output format for plots')
    parser.add_argument('--no_plots', action='store_true',
                       help='Disable plot generation')

    args = parser.parse_args()

    # Initialize analyzer with config
    analyzer = ErrorStatsAnalyzer(config_path=args.config_path)
    
    # Override parameters if provided
    analyzer.plot_format = args.plot_format
    if args.no_plots:
        analyzer.save_plots = False

    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)

    try:
        # Run analysis
        results = analyzer.analyze_files(args.robustness_file, args.summary_file, args.output_dir)
        
        # Print summary
        overall_stats = results.get('overall_stats', {})
        print(f"\nAnalysis Summary:")
        print(f"  Total tools analyzed: {overall_stats.get('total_tools', 0)}")
        print(f"  Average accuracy: {overall_stats.get('avg_accuracy', 0.0):.3f}")
        print(f"  Best performing tool: {overall_stats.get('best_tool', 'N/A')}")
        print(f"  Results saved to: {args.output_dir}")
        
    except Exception as e:
        print(f"Error during analysis: {e}")
        return 1

    return 0


if __name__ == '__main__':
    exit(main())
